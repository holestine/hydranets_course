{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Environment\n",
    "```\n",
    "conda create --name hydranet pytorch torchvision torchaudio pytorch-cuda -c pytorch -c nvidia\n",
    "conda activate hydranet\n",
    "pip install opencv-python matplotlib ipykernel tqdm notebook\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries\n",
    "These are all the libraries you'll need throughout the notebook. The next several sections have code similar to the previous modules in this course in order to define a consistent model architecture for the transfer learning exercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import glob\n",
    "from utils import Normalise, RandomCrop, ToTensor, RandomMirror, InvHuberLoss, AverageMeter, MeanIoU, RMSE\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from model_helpers import Saver, load_state_dict\n",
    "import operator\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import Variable\n",
    "import cv2\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as co\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "from IPython.display import Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset\n",
    "Setup the PyTorch classes to handle the NYU Depth Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_scale = 1.0 / 255\n",
    "depth_scale = 5000.0\n",
    "\n",
    "img_mean = np.array([0.485, 0.456, 0.406])\n",
    "img_std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "normalise_params = [img_scale, img_mean.reshape((1, 1, 3)), img_std.reshape((1, 1, 3)), depth_scale,]\n",
    "\n",
    "transform_common = [Normalise(*normalise_params), ToTensor()]\n",
    "\n",
    "crop_size = 400\n",
    "transform_train = transforms.Compose([RandomMirror(), RandomCrop(crop_size)] + transform_common)\n",
    "transform_val = transforms.Compose(transform_common)\n",
    "\n",
    "train_batch_size = 4\n",
    "val_batch_size = 4\n",
    "train_file = \"train_list_depth.txt\"\n",
    "val_file = \"val_list_depth.txt\"\n",
    "\n",
    "depth = sorted(glob.glob(\"nyud/depth/*.png\"))\n",
    "seg = sorted(glob.glob(\"nyud/masks/*.png\"))\n",
    "images = sorted(glob.glob(\"nyud/rgb/*.png\"))\n",
    "\n",
    "class HydranetDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_file, transform=None):\n",
    "        with open(data_file, \"rb\") as f:\n",
    "            datalist = f.readlines()\n",
    "        self.datalist = [x.decode(\"utf-8\").strip(\"\\n\").split(\"\\t\") for x in datalist]\n",
    "        self.root_dir = \"nyud\"\n",
    "        self.transform = transform\n",
    "        self.masks_names = (\"segm\", \"depth\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datalist)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        abs_paths = [os.path.join(self.root_dir, rpath) for rpath in self.datalist[idx]] # Will output list of nyud/*/00000.png\n",
    "        sample = {}\n",
    "        sample[\"image\"] = np.array(Image.open(abs_paths[0])) #dtype = np.float32\n",
    "\n",
    "        for mask_name, mask_path in zip(self.masks_names, abs_paths[1:]):\n",
    "            mask = np.array(Image.open(mask_path))\n",
    "            assert len(mask.shape) == 2, \"Masks must be encoded without colourmap\"\n",
    "            sample[mask_name] = mask\n",
    "\n",
    "        if self.transform:\n",
    "            sample[\"names\"] = self.masks_names\n",
    "            sample = self.transform(sample)\n",
    "            # the names key can be removed by the transformation\n",
    "            if \"names\" in sample:\n",
    "                del sample[\"names\"]\n",
    "        return sample\n",
    "    \n",
    "#TRAIN DATALOADER\n",
    "trainloader = DataLoader(\n",
    "    HydranetDataset(train_file, transform=transform_train,),\n",
    "    batch_size=train_batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "# VALIDATION DATALOADER\n",
    "valloader = DataLoader(\n",
    "    HydranetDataset(val_file, transform=transform_val,),\n",
    "    batch_size=val_batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=4, \n",
    "    pin_memory=True,\n",
    "    drop_last=False,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder\n",
    "Define all the necessary layers for MobileNet, load the pre-trained weights and freeze them to speed up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1x1(in_channels, out_channels, stride=1, groups=1, bias=False,):\n",
    "    \"1x1 Convolution: Pointwise\"\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, padding=0, bias=bias, groups=groups)\n",
    "\n",
    "def conv3x3(in_channels, out_channels, stride=1, dilation=1, groups=1, bias=False):\n",
    "    \"\"\"3x3 Convolution: Depthwise: \n",
    "    https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "    \"\"\"\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=dilation, dilation=dilation, bias=bias, groups=groups)\n",
    "\n",
    "def batchnorm(num_features):\n",
    "    \"\"\"\n",
    "    https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html\n",
    "    \"\"\"\n",
    "    return nn.BatchNorm2d(num_features, affine=True, eps=1e-5, momentum=0.1)\n",
    "\n",
    "def convbnrelu(in_channels, out_channels, kernel_size, stride=1, groups=1, act=True):\n",
    "    \"conv-batchnorm-relu\"\n",
    "    if act:\n",
    "        return nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=int(kernel_size / 2.), groups=groups, bias=False),\n",
    "                             batchnorm(out_channels),\n",
    "                             nn.ReLU6(inplace=True))\n",
    "    else:\n",
    "        return nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=int(kernel_size / 2.), groups=groups, bias=False),\n",
    "                             batchnorm(out_channels))\n",
    "\n",
    "class InvertedResidualBlock(nn.Module):\n",
    "    \"\"\"Inverted Residual Block from https://arxiv.org/abs/1801.04381\"\"\"\n",
    "    def __init__(self, in_planes, out_planes, expansion_factor, stride=1):\n",
    "        super().__init__() # Python 3\n",
    "        intermed_planes = in_planes * expansion_factor\n",
    "        self.residual = (in_planes == out_planes) and (stride == 1) # Boolean/Condition\n",
    "        self.output = nn.Sequential(convbnrelu(in_planes, intermed_planes, 1),\n",
    "                                    convbnrelu(intermed_planes, intermed_planes, 3, stride=stride, groups=intermed_planes),\n",
    "                                    convbnrelu(intermed_planes, out_planes, 1, act=False))\n",
    "\n",
    "    def forward(self, x):\n",
    "        #residual = x\n",
    "        out = self.output(x)\n",
    "        if self.residual:\n",
    "            return (out + x)#+residual\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "class MobileNetv2(nn.Module):\n",
    "    def __init__(self, return_idx=[6]):\n",
    "        super().__init__()\n",
    "        # expansion rate, output channels, number of repeats, stride\n",
    "        self.mobilenet_config = [\n",
    "        [1, 16, 1, 1],\n",
    "        [6, 24, 2, 2],\n",
    "        [6, 32, 3, 2],\n",
    "        [6, 64, 4, 2],\n",
    "        [6, 96, 3, 1],\n",
    "        [6, 160, 3, 2],\n",
    "        [6, 320, 1, 1],\n",
    "        ]\n",
    "        self.in_channels = 32  # number of input channels\n",
    "        self.num_layers = len(self.mobilenet_config)\n",
    "        self.layer1 = convbnrelu(3, self.in_channels, kernel_size=3, stride=2)\n",
    "    \n",
    "        self.return_idx = [1, 2, 3, 4, 5, 6]\n",
    "        #self.return_idx = make_list(return_idx)\n",
    "\n",
    "        c_layer = 2\n",
    "        for t, c, n, s in self.mobilenet_config:\n",
    "            layers = []\n",
    "            for idx in range(n):\n",
    "                layers.append(InvertedResidualBlock(self.in_channels,c,expansion_factor=t,stride=s if idx == 0 else 1,))\n",
    "                self.in_channels = c\n",
    "            setattr(self, \"layer{}\".format(c_layer), nn.Sequential(*layers))\n",
    "            c_layer += 1\n",
    "\n",
    "        self._out_c = [self.mobilenet_config[idx][1] for idx in self.return_idx] # Output: [24, 32, 64, 96, 160, 320]\n",
    "\n",
    "    def forward(self, x):\n",
    "        outs = []\n",
    "        x = self.layer1(x)\n",
    "        outs.append(self.layer2(x))  # 16, x / 2\n",
    "        outs.append(self.layer3(outs[-1]))  # 24, x / 4\n",
    "        outs.append(self.layer4(outs[-1]))  # 32, x / 8\n",
    "        outs.append(self.layer5(outs[-1]))  # 64, x / 16\n",
    "        outs.append(self.layer6(outs[-1]))  # 96, x / 16\n",
    "        outs.append(self.layer7(outs[-1]))  # 160, x / 32\n",
    "        outs.append(self.layer8(outs[-1]))  # 320, x / 32\n",
    "        return [outs[idx] for idx in self.return_idx]\n",
    "    \n",
    "encoder = MobileNetv2()\n",
    "encoder.load_state_dict(torch.load(\"mobilenetv2-e6e8dd43.pth\"))\n",
    "\n",
    "# Freeze the MobileNet weights\n",
    "for param in encoder.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder\n",
    "Create the decoder and initial model. Most things here are the same as the previous code but notice the parameter *return_backbone* has been added when constructing MTLWRefineNet. Later this will be used to get the outputs from the backbone which will be needed to train a new head, you can see how this is done in the forward method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 3070057 parameters\n"
     ]
    }
   ],
   "source": [
    "def make_list(x):\n",
    "    \"\"\"Returns the given input as a list.\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    elif isinstance(x, tuple):\n",
    "        return list(x)\n",
    "    else:\n",
    "        return [x]\n",
    "    \n",
    "class CRPBlock(nn.Module):\n",
    "    \"\"\"CRP definition\"\"\"\n",
    "    def __init__(self, in_planes, out_planes, n_stages, groups=False):\n",
    "        super().__init__()\n",
    "        for i in range(n_stages):\n",
    "            setattr(self, '{}_{}'.format(i + 1, 'outvar_dimred'),\n",
    "                    conv1x1(in_planes if (i == 0) else out_planes,\n",
    "                            out_planes, stride=1,\n",
    "                            bias=False, groups=in_planes if groups else 1))\n",
    "        self.stride = 1\n",
    "        self.n_stages = n_stages\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=5, stride=1, padding=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        top = x\n",
    "        for i in range(self.n_stages):\n",
    "            top = self.maxpool(top)\n",
    "            top = getattr(self, '{}_{}'.format(i + 1, 'outvar_dimred'))(top)\n",
    "            x = top + x\n",
    "        return x\n",
    "\n",
    "class MTLWRefineNet(nn.Module):\n",
    "    def __init__(self, input_sizes, num_classes, agg_size=256, n_crp=4, return_backbone=False):\n",
    "        super().__init__()\n",
    "\n",
    "        stem_convs = nn.ModuleList()\n",
    "        crp_blocks = nn.ModuleList()\n",
    "        adapt_convs = nn.ModuleList()\n",
    "        heads = nn.ModuleList()\n",
    "\n",
    "        # Reverse since we recover information from the end\n",
    "        input_sizes = list(reversed((input_sizes)))\n",
    "\n",
    "        # No reverse for collapse indices is needed\n",
    "        self.collapse_ind = [[0, 1], [2, 3], 4, 5]\n",
    "\n",
    "        groups = [False] * len(self.collapse_ind)\n",
    "        groups[-1] = True\n",
    "\n",
    "        for size in input_sizes:\n",
    "            stem_convs.append(conv1x1(size, agg_size, bias=False))\n",
    "\n",
    "        for group in groups:\n",
    "            crp_blocks.append(self._make_crp(agg_size, agg_size, n_crp, group))\n",
    "            adapt_convs.append(conv1x1(agg_size, agg_size, bias=False))\n",
    "\n",
    "        self.stem_convs = stem_convs\n",
    "        self.crp_blocks = crp_blocks\n",
    "        self.adapt_convs = adapt_convs[:-1]\n",
    "\n",
    "        num_classes = list(num_classes)\n",
    "        for n_out in num_classes:\n",
    "            heads.append(\n",
    "                nn.Sequential(\n",
    "                    conv1x1(agg_size, agg_size, groups=agg_size, bias=False),\n",
    "                    nn.ReLU6(inplace=False),\n",
    "                    conv3x3(agg_size, n_out, bias=True),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.heads = heads\n",
    "        self.relu = nn.ReLU6(inplace=True)\n",
    "        self.return_backbone = return_backbone\n",
    "\n",
    "    def forward(self, xs):\n",
    "        xs = list(reversed(xs))\n",
    "        for idx, (conv, x) in enumerate(zip(self.stem_convs, xs)):\n",
    "            xs[idx] = conv(x)\n",
    "\n",
    "        # Collapse layers\n",
    "        c_xs = [sum([xs[idx] for idx in make_list(c_idx)]) for c_idx in self.collapse_ind ]\n",
    "\n",
    "        for idx, (crp, x) in enumerate(zip(self.crp_blocks, c_xs)):\n",
    "            if idx == 0:\n",
    "                y = self.relu(x)\n",
    "            else:\n",
    "                y = self.relu(x + y)\n",
    "            y = crp(y)\n",
    "            if idx < (len(c_xs) - 1):\n",
    "                y = self.adapt_convs[idx](y)\n",
    "                y = F.interpolate(\n",
    "                    y,\n",
    "                    size=c_xs[idx + 1].size()[2:],\n",
    "                    mode=\"bilinear\",\n",
    "                    align_corners=True,\n",
    "                )\n",
    "\n",
    "        outs = []\n",
    "        for head in self.heads:\n",
    "            outs.append(head(y))\n",
    "\n",
    "        if self.return_backbone:\n",
    "            outs.append(y)\n",
    "        \n",
    "        return outs\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_crp(in_planes, out_planes, stages, groups):\n",
    "        # Same as previous, but showing the use of a @staticmethod\n",
    "        layers = [CRPBlock(in_planes, out_planes, stages, groups)]\n",
    "        return nn.Sequential(*layers)\n",
    "        \n",
    "        \n",
    "num_classes = (40, 1)\n",
    "decoder = MTLWRefineNet(encoder._out_c, num_classes)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "hydranet = nn.DataParallel(nn.Sequential(encoder, decoder)).to(device)\n",
    "print(\"Model has {} parameters\".format(sum([p.numel() for p in hydranet.parameters()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss\n",
    "Create the loss functions to train the depth and segmentation heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_index = 255\n",
    "ignore_depth = 0\n",
    "\n",
    "crit_segm  = nn.CrossEntropyLoss(ignore_index=ignore_index)\n",
    "crit_depth = InvHuberLoss(ignore_index=ignore_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers\n",
    "Create the encoder and decoder optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_encoder = 1e-2\n",
    "lr_decoder = 1e-3\n",
    "momentum_encoder = 0.9\n",
    "momentum_decoder = 0.9\n",
    "weight_decay_encoder = 1e-5\n",
    "weight_decay_decoder = 1e-5\n",
    "\n",
    "optims = [torch.optim.SGD(encoder.parameters(), lr=lr_encoder, momentum=momentum_encoder, weight_decay=weight_decay_encoder),\n",
    "          torch.optim.SGD(decoder.parameters(), lr=lr_decoder, momentum=momentum_decoder, weight_decay=weight_decay_decoder)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "Define the train and validate methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_vals = (0.0, 10000.0)\n",
    "comp_fns = [operator.gt, operator.lt]\n",
    "ckpt_dir = \"./base_model\"\n",
    "ckpt_path = \"checkpoint.pth.tar\"\n",
    "\n",
    "batch_size = 16\n",
    "val_batch_size = 16\n",
    "val_every = 5\n",
    "loss_coeffs = (0.5, 0.5)\n",
    "n_epochs = 500\n",
    "\n",
    "saver = Saver(\n",
    "    #args=locals(), # Causes issues in vscode\n",
    "    args={\"items\": None},\n",
    "    ckpt_dir=ckpt_dir,\n",
    "    best_val=init_vals,\n",
    "    condition=comp_fns,\n",
    "    save_several_mode=all,\n",
    ")\n",
    "\n",
    "start_epoch, _, state_dict = saver.maybe_load(ckpt_path=ckpt_path, keys_to_load=[\"epoch\", \"best_val\", \"state_dict\"],)\n",
    "load_state_dict(hydranet, state_dict)\n",
    "\n",
    "if start_epoch is None:\n",
    "    start_epoch = 0\n",
    "\n",
    "opt_scheds = []\n",
    "for opt in optims:\n",
    "    opt_scheds.append(torch.optim.lr_scheduler.MultiStepLR(opt, np.arange(start_epoch + 1, n_epochs, 100), gamma=0.1))\n",
    "\n",
    "\n",
    "def train(model, opts, crits, dataloader, loss_coeffs=(1.0,), grad_norm=0.0):\n",
    "    model.train()\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
    "    loss_meter = AverageMeter()\n",
    "    pbar = tqdm(dataloader)\n",
    "\n",
    "    for sample in pbar:\n",
    "        loss = 0.0\n",
    "        input = sample[\"image\"].float().to(device)\n",
    "        targets = [sample[k].to(device) for k in dataloader.dataset.masks_names]\n",
    "        outputs = model(input) # Forward\n",
    "\n",
    "        for out, target, crit, loss_coeff in zip(outputs, targets, crits, loss_coeffs):\n",
    "            loss += loss_coeff * crit(\n",
    "                F.interpolate(\n",
    "                    out, size=target.size()[1:], mode=\"bilinear\", align_corners=False\n",
    "                ).squeeze(dim=1),\n",
    "                target.squeeze(dim=1),\n",
    "            )\n",
    "\n",
    "        # Backward\n",
    "        for opt in opts:\n",
    "            opt.zero_grad()\n",
    "        loss.backward()\n",
    "        if grad_norm > 0.0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_norm)\n",
    "        for opt in opts:\n",
    "            opt.step()\n",
    "\n",
    "        loss_meter.update(loss.item())\n",
    "        pbar.set_description(\n",
    "            \"Loss {:.3f} | Avg. Loss {:.3f}\".format(loss.item(), loss_meter.avg)\n",
    "        )\n",
    "\n",
    "def validate(model, metrics, dataloader):\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
    "    model.eval()\n",
    "    for metric in metrics:\n",
    "        metric.reset()\n",
    "\n",
    "    pbar = tqdm(dataloader)\n",
    "\n",
    "    def get_val(metrics):\n",
    "        results = [(m.name, m.val()) for m in metrics]\n",
    "        names, vals = list(zip(*results))\n",
    "        out = [\"{} : {:4f}\".format(name, val) for name, val in results]\n",
    "        return vals, \" | \".join(out)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sample in pbar:\n",
    "            # Get the Data\n",
    "            input = sample[\"image\"].float().to(device)\n",
    "            targets = [sample[k].to(device) for k in dataloader.dataset.masks_names]\n",
    "\n",
    "            targets = [target.squeeze(dim=1).cpu().numpy() for target in targets]\n",
    "\n",
    "            outputs = model(input) # Forward\n",
    "\n",
    "            # Backward\n",
    "            for out, target, metric in zip(outputs, targets, metrics):\n",
    "                metric.update(\n",
    "                    F.interpolate(out, size=target.shape[1:], mode=\"bilinear\", align_corners=False)\n",
    "                    .squeeze(dim=1)\n",
    "                    .cpu()\n",
    "                    .numpy(),\n",
    "                    target,\n",
    "                )\n",
    "            pbar.set_description(get_val(metrics)[1])\n",
    "    vals, _ = get_val(metrics)\n",
    "    print(\"----\" * 5)\n",
    "    return vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the initial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 1.092 | Avg. Loss 1.394: 100%|██████████| 198/198 [00:15<00:00, 13.14it/s]\n",
      "meaniou : 0.075640 | rmse : 0.979666: 100%|██████████| 164/164 [01:49<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Epoch 206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 1.207 | Avg. Loss 1.376: 100%|██████████| 198/198 [00:15<00:00, 13.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 1.318 | Avg. Loss 1.359: 100%|██████████| 198/198 [00:15<00:00, 13.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 1.482 | Avg. Loss 1.340: 100%|██████████| 198/198 [00:15<00:00, 13.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 1.290 | Avg. Loss 1.366: 100%|██████████| 198/198 [00:15<00:00, 13.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 1.457 | Avg. Loss 1.269: 100%|██████████| 198/198 [00:15<00:00, 13.11it/s]\n",
      "meaniou : 0.098136 | rmse : 0.908937: 100%|██████████| 164/164 [01:49<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Epoch 211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 1.100 | Avg. Loss 1.260: 100%|██████████| 198/198 [00:15<00:00, 13.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 1.354 | Avg. Loss 1.290: 100%|██████████| 198/198 [00:15<00:00, 13.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 1.230 | Avg. Loss 1.322: 100%|██████████| 198/198 [00:15<00:00, 13.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 1.182 | Avg. Loss 1.280: 100%|██████████| 198/198 [00:15<00:00, 13.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 1.078 | Avg. Loss 1.220: 100%|██████████| 198/198 [00:15<00:00, 13.11it/s]\n",
      "meaniou : 0.114806 | rmse : 0.912386: 100%|██████████| 164/164 [01:49<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Epoch 216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 1.212 | Avg. Loss 1.218: 100%|██████████| 198/198 [00:14<00:00, 13.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.972 | Avg. Loss 1.193: 100%|██████████| 198/198 [00:15<00:00, 13.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 1.401 | Avg. Loss 1.266: 100%|██████████| 198/198 [00:15<00:00, 13.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 1.288 | Avg. Loss 1.161: 100%|██████████| 198/198 [00:15<00:00, 13.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 1.205 | Avg. Loss 1.159: 100%|██████████| 198/198 [00:15<00:00, 13.17it/s]\n",
      "meaniou : 0.111765 | rmse : 0.862699: 100%|██████████| 164/164 [01:48<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Epoch 221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 1.382 | Avg. Loss 1.158: 100%|██████████| 198/198 [00:15<00:00, 13.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 1.035 | Avg. Loss 1.167: 100%|██████████| 198/198 [00:15<00:00, 13.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 1.485 | Avg. Loss 1.166: 100%|██████████| 198/198 [00:15<00:00, 13.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 1.288 | Avg. Loss 1.188: 100%|██████████| 198/198 [00:15<00:00, 13.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 1.362 | Avg. Loss 1.132: 100%|██████████| 198/198 [00:15<00:00, 13.06it/s]\n",
      "meaniou : 0.128287 | rmse : 0.886979: 100%|██████████| 164/164 [01:50<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Epoch 226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 1.140 | Avg. Loss 1.183: 100%|██████████| 198/198 [00:15<00:00, 13.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 1.158 | Avg. Loss 1.172: 100%|██████████| 198/198 [00:15<00:00, 13.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.916 | Avg. Loss 1.118: 100%|██████████| 198/198 [00:15<00:00, 13.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 1.302 | Avg. Loss 1.180: 100%|██████████| 198/198 [00:15<00:00, 13.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 1.081 | Avg. Loss 1.112: 100%|██████████| 198/198 [00:15<00:00, 13.17it/s]\n",
      "meaniou : 0.147989 | rmse : 0.893857: 100%|██████████| 164/164 [01:47<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Epoch 231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.962 | Avg. Loss 1.118: 100%|██████████| 198/198 [00:14<00:00, 13.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 1.229 | Avg. Loss 1.079: 100%|██████████| 198/198 [00:15<00:00, 13.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.881 | Avg. Loss 1.106: 100%|██████████| 198/198 [00:15<00:00, 13.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 1.126 | Avg. Loss 1.062: 100%|██████████| 198/198 [00:15<00:00, 13.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 1.170 | Avg. Loss 1.054: 100%|██████████| 198/198 [00:15<00:00, 13.10it/s]\n",
      "meaniou : 0.147022 | rmse : 0.909321: 100%|██████████| 164/164 [01:47<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Epoch 236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 1.235 | Avg. Loss 1.068: 100%|██████████| 198/198 [00:15<00:00, 13.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 1.177 | Avg. Loss 1.086: 100%|██████████| 198/198 [00:15<00:00, 13.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 1.044 | Avg. Loss 1.060: 100%|██████████| 198/198 [00:15<00:00, 13.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 1.042 | Avg. Loss 1.014: 100%|██████████| 198/198 [00:15<00:00, 13.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.945 | Avg. Loss 1.052: 100%|██████████| 198/198 [00:15<00:00, 13.17it/s]\n",
      "meaniou : 0.163417 | rmse : 0.826619: 100%|██████████| 164/164 [01:49<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Epoch 241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 1.094 | Avg. Loss 1.009: 100%|██████████| 198/198 [00:14<00:00, 13.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.942 | Avg. Loss 1.092: 100%|██████████| 198/198 [00:15<00:00, 13.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 1.003 | Avg. Loss 1.033: 100%|██████████| 198/198 [00:15<00:00, 13.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.866 | Avg. Loss 1.022: 100%|██████████| 198/198 [00:15<00:00, 12.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.984 | Avg. Loss 1.039: 100%|██████████| 198/198 [00:15<00:00, 13.09it/s]\n",
      "meaniou : 0.175906 | rmse : 0.816754: 100%|██████████| 164/164 [01:49<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Epoch 246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.968 | Avg. Loss 0.973: 100%|██████████| 198/198 [00:15<00:00, 13.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.937 | Avg. Loss 1.031: 100%|██████████| 198/198 [00:15<00:00, 13.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.910 | Avg. Loss 1.006: 100%|██████████| 198/198 [00:15<00:00, 13.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 1.152 | Avg. Loss 0.991: 100%|██████████| 198/198 [00:15<00:00, 13.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.875 | Avg. Loss 0.994: 100%|██████████| 198/198 [00:15<00:00, 13.19it/s]\n",
      "meaniou : 0.186285 | rmse : 0.829028: 100%|██████████| 164/164 [01:50<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Epoch 251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.887 | Avg. Loss 0.960: 100%|██████████| 198/198 [00:15<00:00, 13.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.891 | Avg. Loss 0.999: 100%|██████████| 198/198 [00:15<00:00, 13.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.898 | Avg. Loss 0.971: 100%|██████████| 198/198 [00:15<00:00, 13.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.964 | Avg. Loss 0.997: 100%|██████████| 198/198 [00:15<00:00, 13.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 1.109 | Avg. Loss 0.960: 100%|██████████| 198/198 [00:15<00:00, 13.04it/s]\n",
      "meaniou : 0.190064 | rmse : 0.835472: 100%|██████████| 164/164 [01:50<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Epoch 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 1.118 | Avg. Loss 0.940: 100%|██████████| 198/198 [00:15<00:00, 13.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 1.086 | Avg. Loss 0.962: 100%|██████████| 198/198 [00:14<00:00, 13.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 1.076 | Avg. Loss 0.929: 100%|██████████| 198/198 [00:15<00:00, 13.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.804 | Avg. Loss 0.947: 100%|██████████| 198/198 [00:15<00:00, 13.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.872 | Avg. Loss 0.919: 100%|██████████| 198/198 [00:15<00:00, 13.16it/s]\n",
      "meaniou : 0.188537 | rmse : 0.820088: 100%|██████████| 164/164 [01:50<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Epoch 261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.806 | Avg. Loss 0.903: 100%|██████████| 198/198 [00:15<00:00, 13.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 1.185 | Avg. Loss 0.977: 100%|██████████| 198/198 [00:15<00:00, 13.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.965 | Avg. Loss 0.917: 100%|██████████| 198/198 [00:15<00:00, 13.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.971 | Avg. Loss 0.917: 100%|██████████| 198/198 [00:15<00:00, 13.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.998 | Avg. Loss 0.908: 100%|██████████| 198/198 [00:15<00:00, 13.13it/s]\n",
      "meaniou : 0.208602 | rmse : 0.808919: 100%|██████████| 164/164 [01:50<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Epoch 266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.843 | Avg. Loss 0.922: 100%|██████████| 198/198 [00:15<00:00, 13.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.860 | Avg. Loss 0.926: 100%|██████████| 198/198 [00:15<00:00, 13.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.795 | Avg. Loss 0.918: 100%|██████████| 198/198 [00:15<00:00, 13.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 1.115 | Avg. Loss 0.948: 100%|██████████| 198/198 [00:15<00:00, 13.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 1.076 | Avg. Loss 0.909: 100%|██████████| 198/198 [00:14<00:00, 13.22it/s]\n",
      "meaniou : 0.199809 | rmse : 0.807512: 100%|██████████| 164/164 [01:50<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Epoch 271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 1.018 | Avg. Loss 0.893: 100%|██████████| 198/198 [00:15<00:00, 13.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.826 | Avg. Loss 0.920: 100%|██████████| 198/198 [00:15<00:00, 13.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.879 | Avg. Loss 0.896: 100%|██████████| 198/198 [00:15<00:00, 13.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.722 | Avg. Loss 0.861: 100%|██████████| 198/198 [00:15<00:00, 13.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.740 | Avg. Loss 0.937: 100%|██████████| 198/198 [00:15<00:00, 13.13it/s]\n",
      "meaniou : 0.202367 | rmse : 0.810330: 100%|██████████| 164/164 [01:51<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Epoch 276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.762 | Avg. Loss 0.928: 100%|██████████| 198/198 [00:15<00:00, 13.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.865 | Avg. Loss 0.908: 100%|██████████| 198/198 [00:15<00:00, 13.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.784 | Avg. Loss 0.861: 100%|██████████| 198/198 [00:15<00:00, 13.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.907 | Avg. Loss 0.868: 100%|██████████| 198/198 [00:15<00:00, 13.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.956 | Avg. Loss 0.895: 100%|██████████| 198/198 [00:15<00:00, 13.11it/s]\n",
      "meaniou : 0.216132 | rmse : 0.811019: 100%|██████████| 164/164 [01:51<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Epoch 281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.945 | Avg. Loss 0.879: 100%|██████████| 198/198 [00:15<00:00, 13.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.637 | Avg. Loss 0.853: 100%|██████████| 198/198 [00:15<00:00, 13.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.785 | Avg. Loss 0.850: 100%|██████████| 198/198 [00:15<00:00, 13.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 1.060 | Avg. Loss 0.877: 100%|██████████| 198/198 [00:15<00:00, 13.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.743 | Avg. Loss 0.812: 100%|██████████| 198/198 [00:15<00:00, 13.11it/s]\n",
      "meaniou : 0.220676 | rmse : 0.817767: 100%|██████████| 164/164 [01:50<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Epoch 286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.771 | Avg. Loss 0.816: 100%|██████████| 198/198 [00:15<00:00, 13.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.698 | Avg. Loss 0.877: 100%|██████████| 198/198 [00:15<00:00, 13.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.940 | Avg. Loss 0.851: 100%|██████████| 198/198 [00:15<00:00, 13.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.874 | Avg. Loss 0.836: 100%|██████████| 198/198 [00:15<00:00, 13.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.932 | Avg. Loss 0.901: 100%|██████████| 198/198 [00:15<00:00, 13.10it/s]\n",
      "meaniou : 0.220818 | rmse : 0.791059: 100%|██████████| 164/164 [01:50<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Epoch 291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.612 | Avg. Loss 0.819: 100%|██████████| 198/198 [00:14<00:00, 13.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 1.027 | Avg. Loss 0.853: 100%|██████████| 198/198 [00:14<00:00, 13.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.949 | Avg. Loss 0.844: 100%|██████████| 198/198 [00:15<00:00, 13.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.842 | Avg. Loss 0.814: 100%|██████████| 198/198 [00:15<00:00, 13.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.794 | Avg. Loss 0.806: 100%|██████████| 198/198 [00:15<00:00, 13.20it/s]\n",
      "meaniou : 0.228635 | rmse : 0.796994: 100%|██████████| 164/164 [01:53<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Epoch 296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.889 | Avg. Loss 0.785: 100%|██████████| 198/198 [00:14<00:00, 13.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.874 | Avg. Loss 0.800: 100%|██████████| 198/198 [00:15<00:00, 13.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.667 | Avg. Loss 0.800: 100%|██████████| 198/198 [00:15<00:00, 13.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.768 | Avg. Loss 0.779: 100%|██████████| 198/198 [00:15<00:00, 13.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.889 | Avg. Loss 0.798: 100%|██████████| 198/198 [00:14<00:00, 13.22it/s]\n",
      "meaniou : 0.213478 | rmse : 0.842136: 100%|██████████| 164/164 [01:50<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Epoch 301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.933 | Avg. Loss 0.780: 100%|██████████| 198/198 [00:15<00:00, 13.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.977 | Avg. Loss 0.808: 100%|██████████| 198/198 [00:14<00:00, 13.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.981 | Avg. Loss 0.818: 100%|██████████| 198/198 [00:15<00:00, 13.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.664 | Avg. Loss 0.782: 100%|██████████| 198/198 [00:14<00:00, 13.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.842 | Avg. Loss 0.779: 100%|██████████| 198/198 [00:15<00:00, 13.18it/s]\n",
      "meaniou : 0.211166 | rmse : 0.798123: 100%|██████████| 164/164 [01:51<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Epoch 306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.838 | Avg. Loss 0.778: 100%|██████████| 198/198 [00:14<00:00, 13.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.761 | Avg. Loss 0.785: 100%|██████████| 198/198 [00:14<00:00, 13.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.766 | Avg. Loss 0.798: 100%|██████████| 198/198 [00:14<00:00, 13.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.806 | Avg. Loss 0.777: 100%|██████████| 198/198 [00:15<00:00, 13.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.720 | Avg. Loss 0.773: 100%|██████████| 198/198 [00:15<00:00, 13.06it/s]\n",
      "meaniou : 0.234403 | rmse : 0.802896: 100%|██████████| 164/164 [01:45<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Epoch 311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.706 | Avg. Loss 0.795: 100%|██████████| 198/198 [00:14<00:00, 13.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.688 | Avg. Loss 0.749: 100%|██████████| 198/198 [00:14<00:00, 13.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.622 | Avg. Loss 0.772: 100%|██████████| 198/198 [00:15<00:00, 13.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.688 | Avg. Loss 0.785: 100%|██████████| 198/198 [00:15<00:00, 13.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.738 | Avg. Loss 0.777: 100%|██████████| 198/198 [00:15<00:00, 13.16it/s]\n",
      "meaniou : 0.234400 | rmse : 0.814736: 100%|██████████| 164/164 [01:48<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Epoch 316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.731 | Avg. Loss 0.746: 100%|██████████| 198/198 [00:15<00:00, 13.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.740 | Avg. Loss 0.733: 100%|██████████| 198/198 [00:15<00:00, 13.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.624 | Avg. Loss 0.771: 100%|██████████| 198/198 [00:15<00:00, 13.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.808 | Avg. Loss 0.749: 100%|██████████| 198/198 [00:15<00:00, 13.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.609 | Avg. Loss 0.752: 100%|██████████| 198/198 [00:15<00:00, 13.13it/s]\n",
      "meaniou : 0.234563 | rmse : 0.822322: 100%|██████████| 164/164 [01:51<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Epoch 321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.746 | Avg. Loss 0.726: 100%|██████████| 198/198 [00:14<00:00, 13.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.682 | Avg. Loss 0.739: 100%|██████████| 198/198 [00:15<00:00, 13.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 1.023 | Avg. Loss 0.764: 100%|██████████| 198/198 [00:15<00:00, 13.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.686 | Avg. Loss 0.766: 100%|██████████| 198/198 [00:15<00:00, 13.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.786 | Avg. Loss 0.712: 100%|██████████| 198/198 [00:15<00:00, 13.16it/s]\n",
      "meaniou : 0.233812 | rmse : 0.825800: 100%|██████████| 164/164 [01:48<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Epoch 326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.604 | Avg. Loss 0.696: 100%|██████████| 198/198 [00:15<00:00, 13.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.627 | Avg. Loss 0.738: 100%|██████████| 198/198 [00:15<00:00, 13.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.718 | Avg. Loss 0.722: 100%|██████████| 198/198 [00:15<00:00, 13.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.623 | Avg. Loss 0.729: 100%|██████████| 198/198 [00:15<00:00, 13.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.660 | Avg. Loss 0.707: 100%|██████████| 198/198 [00:15<00:00, 13.07it/s]\n",
      "meaniou : 0.134420 | rmse : 1.024391:   2%|▏         | 4/164 [00:03<01:55,  1.38it/s]"
     ]
    }
   ],
   "source": [
    "for i in range(start_epoch, n_epochs + 1):\n",
    "    \n",
    "    print(\"Epoch {:d}\".format(i))\n",
    "    train(hydranet, optims, [crit_segm, crit_depth], trainloader, loss_coeffs)\n",
    "\n",
    "    for sched in opt_scheds:\n",
    "        sched.step()\n",
    "    \n",
    "    if i % val_every == 0:\n",
    "        metrics = [MeanIoU(num_classes[0]),RMSE(ignore_val=ignore_depth),]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            vals = validate(hydranet, metrics, valloader)\n",
    "        saver.maybe_save(new_val=vals, dict_to_save={\"state_dict\": hydranet.state_dict(), \"epoch\": i})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "Let's load the best model and visualize what it learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'base_model/checkpoint.pth.tar'\n",
    "model = nn.DataParallel(nn.Sequential(encoder, decoder)).to(device)\n",
    "model.load_state_dict(torch.load(checkpoint)['state_dict'])\n",
    "res = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get and display a random image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing and post-processing constants \n",
    "IMG_SCALE  = 1./255\n",
    "IMG_MEAN = np.array([0.485, 0.456, 0.406]).reshape((1, 1, 3))\n",
    "IMG_STD = np.array([0.229, 0.224, 0.225]).reshape((1, 1, 3))\n",
    "\n",
    "def prepare_img(img):\n",
    "    return (img * IMG_SCALE - IMG_MEAN) / IMG_STD\n",
    "\n",
    "CMAP = np.load('cmap_nyud.npy')\n",
    "NUM_CLASSES = 40\n",
    "\n",
    "images_files = glob.glob('data/*.png')\n",
    "idx = np.random.randint(0, len(images_files))\n",
    "\n",
    "img_path = images_files[idx]\n",
    "img = np.array(Image.open(img_path))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get and display the model's output for that image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(img):\n",
    "    with torch.no_grad():\n",
    "        img_var = Variable(torch.from_numpy(prepare_img(img).transpose(2, 0, 1)[None]), requires_grad=False).float()\n",
    "        if torch.cuda.is_available():\n",
    "            img_var = img_var.cuda()\n",
    "        segm, depth = model(img_var)\n",
    "        segm = cv2.resize(segm[0, :NUM_CLASSES].cpu().data.numpy().transpose(1, 2, 0),\n",
    "                        img.shape[:2][::-1],\n",
    "                        interpolation=cv2.INTER_CUBIC)\n",
    "        depth = cv2.resize(depth[0, 0].cpu().data.numpy(),\n",
    "                        img.shape[:2][::-1],\n",
    "                        interpolation=cv2.INTER_CUBIC)\n",
    "        segm = CMAP[segm.argmax(axis=2)].astype(np.uint8)\n",
    "        depth = np.abs(depth)\n",
    "        return depth, segm\n",
    "    \n",
    "depth, segm = pipeline(img)\n",
    "\n",
    "f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(30,20))\n",
    "ax1.imshow(img)\n",
    "ax1.set_title('Original', fontsize=30)\n",
    "ax2.imshow(segm)\n",
    "ax2.set_title('Predicted Segmentation', fontsize=30)\n",
    "ax3.imshow(depth, cmap=\"plasma\", vmin=0, vmax=80)\n",
    "ax3.set_title(\"Predicted Depth\", fontsize=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's normalize the depth and visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def depth_to_rgb(depth):\n",
    "    normalizer = co.Normalize()\n",
    "    mapper = cm.ScalarMappable(norm=normalizer, cmap='plasma')\n",
    "    colormapped_im = (mapper.to_rgba(depth)[:, :, :3] * 255).astype(np.uint8)\n",
    "    return colormapped_im\n",
    "\n",
    "depth_rgb = depth_to_rgb(depth)\n",
    "\n",
    "new_img = np.vstack((img, segm, depth_rgb))\n",
    "plt.imshow(new_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it looks as a video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_files = sorted(glob.glob(\"data/*.png\"))\n",
    "\n",
    "# Set the Model to Eval on GPU\n",
    "if torch.cuda.is_available():\n",
    "    _ = model.cuda()\n",
    "_ = model.eval()\n",
    "\n",
    "# Run the pipeline\n",
    "result_video = []\n",
    "for idx, img_path in enumerate(video_files):\n",
    "    image = np.array(Image.open(img_path))\n",
    "    h, w, _ = image.shape \n",
    "    depth, seg = pipeline(image)\n",
    "    result_video.append(cv2.cvtColor(cv2.vconcat([image, seg, depth_to_rgb(depth)]), cv2.COLOR_BGR2RGB))\n",
    "\n",
    "if not os.path.exists('output'):\n",
    "    os.makedirs('output')\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter('output/out.mp4', fourcc, 15, (w,3*h))\n",
    "\n",
    "for i in range(len(result_video)):\n",
    "    out.write(result_video[i])\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extend the Model\n",
    "There are many ways to transfer weights from one model to another, this approach uses composition to get backbone output from the base model to use it as input to an additional head. The base model is constructed in the same way however the parameter *return_backbone* is now set to true. This model is loaded with weights from the checkpoint and they are frozen to prevent changes that would affect the heads on the base model which also allows us to use larger batch sizes and ultimately train faster. Next the architecture of the new head is defined and the forward method shows how the backbone output is taken from the base model and passed to the new head. In order to minimize the additional complexities the new head is trained on the existing depth data however it allows us to experiment with different loss functions and optimizers than those used to train the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = \"./extended_model\"\n",
    "ckpt_path = \"checkpoint.pth.tar\"\n",
    "\n",
    "saver = Saver(\n",
    "    #args=locals(), # Causes issues in vscode\n",
    "    args={\"items\": None},\n",
    "    ckpt_dir=ckpt_dir,\n",
    "    best_val=init_vals,\n",
    "    condition=comp_fns,\n",
    "    save_several_mode=all,\n",
    ")\n",
    "\n",
    "num_classes = (40, 1)\n",
    "backbone_path = \"./base_model/checkpoint.pth.tar\"\n",
    "\n",
    "class Extended_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Create an instance of the base model and load the saved weights\n",
    "        encoder = MobileNetv2()\n",
    "        decoder = MTLWRefineNet(encoder._out_c, num_classes, return_backbone=True)        \n",
    "        self.net1 = nn.DataParallel(nn.Sequential(encoder, decoder))\n",
    "        self.net1.load_state_dict(torch.load(backbone_path)['state_dict'])\n",
    "\n",
    "        # Freeze the trained weights\n",
    "        for param in self.net1.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        conv1 = nn.Conv2d(256,128,1,1)\n",
    "        conv2 = nn.Conv2d(128,64,3,1)\n",
    "        conv3 = nn.Conv2d(64,1,3,1)\n",
    "        self.net2 = nn.DataParallel(nn.Sequential(conv1, nn.ReLU6(inplace=True), conv2, nn.ReLU6(inplace=True), conv3, nn.ReLU6(inplace=True)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net1(x)\n",
    "        y   = self.net2(out[-1])\n",
    "        return out, y\n",
    "    \n",
    "\n",
    "hydranet = nn.DataParallel(Extended_Model()).to(device)\n",
    "\n",
    "print(\"Model has {} parameters\".format(sum([p.numel() for p in hydranet.parameters()])))\n",
    "\n",
    "start_epoch, _, state_dict = saver.maybe_load(ckpt_path=ckpt_path, keys_to_load=[\"epoch\", \"best_val\", \"state_dict\"],)\n",
    "load_state_dict(hydranet, state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The display pipeline is modified to process the new head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(img):\n",
    "    with torch.no_grad():\n",
    "        img_var = Variable(torch.from_numpy(prepare_img(img).transpose(2, 0, 1)[None]), requires_grad=False).float()\n",
    "        if torch.cuda.is_available():\n",
    "            img_var = img_var.cuda()\n",
    "        head1, depth2 = hydranet(img_var)    # The new forward method returns the old heads with the backbone outputs \n",
    "        segm, depth1 = head1[0], head1[1]    # and the new depth output, pull off what needs to be processed\n",
    "        segm = cv2.resize(segm[0, :NUM_CLASSES].cpu().data.numpy().transpose(1, 2, 0),\n",
    "                        img.shape[:2][::-1],\n",
    "                        interpolation=cv2.INTER_CUBIC)\n",
    "        depth1 = cv2.resize(depth1[0, 0].cpu().data.numpy(),\n",
    "                        img.shape[:2][::-1],\n",
    "                        interpolation=cv2.INTER_CUBIC)\n",
    "        depth2 = cv2.resize(depth2[0, 0].cpu().data.numpy(),\n",
    "                        img.shape[:2][::-1],\n",
    "                        interpolation=cv2.INTER_CUBIC)\n",
    "        segm = CMAP[segm.argmax(axis=2)].astype(np.uint8)\n",
    "        depth1 = np.abs(depth1)\n",
    "        depth2 = np.abs(depth2)\n",
    "        return depth1, depth2, segm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the output before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth1, depth2, segm = pipeline(img)\n",
    "\n",
    "print(depth1.shape)\n",
    "print(depth2.shape)\n",
    "print(segm.shape)\n",
    "\n",
    "depth1_rgb = depth_to_rgb(depth1)\n",
    "depth2_rgb = depth_to_rgb(depth2)\n",
    "\n",
    "untrained_img = np.vstack((img, segm, depth1_rgb, depth2_rgb))\n",
    "plt.imshow(untrained_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_files = sorted(glob.glob(\"data/*.png\"))\n",
    "\n",
    "# Set the Model to Eval on GPU\n",
    "if torch.cuda.is_available():\n",
    "    _ = hydranet.cuda()\n",
    "_ = hydranet.eval()\n",
    "\n",
    "\n",
    "# Run the pipeline\n",
    "result_video = []\n",
    "for idx, img_path in enumerate(video_files):\n",
    "    image = np.array(Image.open(img_path))\n",
    "    h, w, _ = image.shape \n",
    "    depth1, depth2, seg = pipeline(image)\n",
    "    result_video.append(cv2.cvtColor(cv2.vconcat([image, seg, depth_to_rgb(depth1), depth_to_rgb(depth2)]), cv2.COLOR_BGR2RGB))\n",
    "\n",
    "if not os.path.exists('output'):\n",
    "    os.makedirs('output')\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter('output/out2.mp4', fourcc, 15, (w,4*h))\n",
    "\n",
    "for i in range(len(result_video)):\n",
    "    out.write(result_video[i])\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Extended Model\n",
    "The train and validate methods have been modified to work with the new output structure of the forward method and to calculate the metrics based on the new depth head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_vals = (0.0, 10000.0)\n",
    "comp_fns = [operator.gt, operator.lt]\n",
    "ckpt_dir = \"./extended_model\"\n",
    "ckpt_path = \"./extended/checkpoint.pth.tar\"\n",
    "\n",
    "saver = Saver(\n",
    "    #args=locals(),\n",
    "    args={\"items\": None},\n",
    "    ckpt_dir=ckpt_dir,\n",
    "    best_val=init_vals,\n",
    "    condition=comp_fns,\n",
    "    save_several_mode=all,\n",
    ")\n",
    "\n",
    "n_epochs = 200\n",
    "\n",
    "hydranet = nn.DataParallel(Extended_Model())\n",
    "\n",
    "print(\"Model has {} parameters\".format(sum([p.numel() for p in hydranet.parameters()])))\n",
    "\n",
    "start_epoch, _, state_dict = saver.maybe_load(ckpt_path=ckpt_path, keys_to_load=[\"epoch\", \"best_val\", \"state_dict\"],)\n",
    "load_state_dict(hydranet, state_dict)\n",
    "\n",
    "if start_epoch is None:\n",
    "    start_epoch = 0\n",
    "\n",
    "def train(model, opts, crits, dataloader, loss_coeffs=(1.0,), grad_norm=0.0):\n",
    "    model.train()\n",
    "\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
    "    loss_meter = AverageMeter()\n",
    "    pbar = tqdm(dataloader)\n",
    "\n",
    "    for sample in pbar:\n",
    "        loss = 0.0\n",
    "        input = sample[\"image\"].float().to(device)\n",
    "        targets = [sample[k].to(device) for k in dataloader.dataset.masks_names]\n",
    "        outputs = model(input)\n",
    "\n",
    "        loss = crits[0](\n",
    "                F.interpolate(\n",
    "                    outputs[-1], size=targets[-1].size()[1:], mode=\"bilinear\", align_corners=False\n",
    "                ).squeeze(dim=1),\n",
    "                targets[-1].squeeze(dim=1),\n",
    "            )\n",
    "        \n",
    "        opts[0].zero_grad()\n",
    "        loss.backward()\n",
    "        if grad_norm > 0.0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_norm)\n",
    "        opts[0].step()\n",
    "\n",
    "        loss_meter.update(loss.item())\n",
    "        pbar.set_description(\n",
    "            \"Loss {:.3f} | Avg. Loss {:.3f}\".format(loss.item(), loss_meter.avg)\n",
    "        )\n",
    "\n",
    "def validate(model, metrics, dataloader):\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
    "    model.eval()\n",
    "    for metric in metrics:\n",
    "        metric.reset()\n",
    "\n",
    "    pbar = tqdm(dataloader)\n",
    "\n",
    "    def get_val(metrics):\n",
    "        results = [(m.name, m.val()) for m in metrics]\n",
    "        names, vals = list(zip(*results))\n",
    "        out = [\"{} : {:4f}\".format(name, val) for name, val in results]\n",
    "        return vals, \" | \".join(out)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sample in pbar:\n",
    "            # Get the Data\n",
    "            input = sample[\"image\"].float().to(device)\n",
    "            targets = [sample[k].to(device) for k in dataloader.dataset.masks_names]\n",
    "\n",
    "            targets = [target.squeeze(dim=1).cpu().numpy() for target in targets]\n",
    "\n",
    "            # Forward\n",
    "            outputs, depth2 = model(input)\n",
    "            outputs.pop(-1)                 # Remove the backbone from the outputs\n",
    "            outputs[1] = depth2             # Use the new depth head for metric calculation\n",
    "\n",
    "            # Backward\n",
    "            for out, target, metric in zip(outputs, targets, metrics):\n",
    "                metric.update(\n",
    "                    F.interpolate(out, size=target.shape[1:], mode=\"bilinear\", align_corners=False)\n",
    "                    .squeeze(dim=1)\n",
    "                    .cpu()\n",
    "                    .numpy(),\n",
    "                    target,\n",
    "                )\n",
    "            pbar.set_description(get_val(metrics)[1])\n",
    "    vals, _ = get_val(metrics)\n",
    "    print(\"----\" * 5)\n",
    "    return vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(start_epoch)\n",
    "batch_size = 16\n",
    "val_batch_size = 16\n",
    "val_every = 5\n",
    "\n",
    "# Try a different optimizer and loss function\n",
    "optims = [torch.optim.Adam(hydranet.module.net2.parameters(), lr=lr_decoder)]\n",
    "crit_depth = nn.MSELoss()\n",
    "\n",
    "opt_scheds = []\n",
    "for opt in optims:\n",
    "    opt_scheds.append(torch.optim.lr_scheduler.MultiStepLR(opt, np.arange(start_epoch + 1, n_epochs, 100), gamma=0.1))\n",
    "          \n",
    "for i in range(start_epoch, n_epochs + 1):\n",
    "    \n",
    "    print(\"Epoch {:d}\".format(i))\n",
    "    train(hydranet, optims, [crit_depth], trainloader, loss_coeffs)\n",
    "\n",
    "    for sched in opt_scheds:\n",
    "        sched.step()\n",
    "    \n",
    "    if i % val_every == 0:\n",
    "        metrics = [MeanIoU(num_classes[0]),RMSE(ignore_val=ignore_depth),]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            vals = validate(hydranet, metrics, valloader)\n",
    "        saver.maybe_save(new_val=vals, dict_to_save={\"state_dict\": hydranet.state_dict(), \"epoch\": i})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Results\n",
    "Now that it's trained let's see if the output looks better. Notice that the output for the base model's heads are unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth1, depth2, segm = pipeline(img)\n",
    "depth1_rgb = depth_to_rgb(depth1)\n",
    "depth2_rgb = depth_to_rgb(depth2)\n",
    "\n",
    "new_img = np.hstack(untrained_img, np.vstack((img, segm, depth1_rgb, depth2_rgb)))\n",
    "plt.imshow(new_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_files = sorted(glob.glob(\"data/*.png\"))\n",
    "\n",
    "# Set the Model to Eval on GPU\n",
    "if torch.cuda.is_available():\n",
    "    _ = hydranet.cuda()\n",
    "_ = hydranet.eval()\n",
    "\n",
    "\n",
    "# Run the pipeline\n",
    "result_video = []\n",
    "for idx, img_path in enumerate(video_files):\n",
    "    image = np.array(Image.open(img_path))\n",
    "    h, w, _ = image.shape \n",
    "    depth1, depth2, seg = pipeline(image)\n",
    "    result_video.append(cv2.cvtColor(cv2.vconcat([image, seg, depth_to_rgb(depth1), depth_to_rgb(depth2)]), cv2.COLOR_BGR2RGB))\n",
    "\n",
    "if not os.path.exists('output'):\n",
    "    os.makedirs('output')\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "#fourcc = cv2.VideoWriter_fourcc('M','J','P','G')\n",
    "out = cv2.VideoWriter('output/out3.mp4', fourcc, 15, (w,4*h))\n",
    "\n",
    "for i in range(len(result_video)):\n",
    "    out.write(result_video[i])\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<video width=\"800\" controls>\n",
    "    <source src=\"./output/out3.mp4\" type=\"video/mp4\">\n",
    "</video>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
